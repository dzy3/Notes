提高深度神经网络性能最直接的方法是增加它们的规模。
这包括增加网络和its的深度(层数)
宽度:每个关卡的单位数量。
这是一种简单又安全的培训方法
模型，特别是考虑到可获得的大量标记训练数据。
然而,这
简单的解决方案有两个主要缺点。
网络尺寸越大，参数的个数就越多，这就使得扩大后的网络更大
容易过度拟合，特别是在训练集中标记的样本数量有限的情况下。
这可能成为一个主要的瓶颈，因为创建高质量的训练集可能是棘手的和昂贵的，特别是当需要专家人工评分来区分细粒度的时候
如所示，ImageNet中的视觉分类(甚至在1000类ILSVRC子集中)
由图1所示。
统一增加网络规模的另一个缺点是大大增加了对计算机网络资源的使用。
例如，在深度视觉网络中，如果两个卷积层被连接起来，
任何滤波器数目的均匀增加都会导致计算量的二次增加。
如果
增加的容量被低效地使用(例如，如果大多数权重最终接近于零)，
这样就浪费了大量的计算。
由于在实践中，计算预算总是有限的，因此
高效地分配计算资源比不加选择地增加规模更可取
当主要目标是提高结果的质量时。
解决这两个问题的根本途径将是最终从完全联系的方式转变
到稀疏连接的架构，甚至在卷积内部。
除了模仿生物
系统，这也将有更坚实的理论基础的优势，由于基础 Arora等人[2]的突破性工作。
他们的主要结果是，如果概率分布
数据集可以用一个大的、非常稀疏的深度神经网络来表示，然后是最优网络
通过分析激活的相关统计信息，可以一层一层地构造拓扑结构
最后一层和聚类神经元的高度相关输出。
虽然严格的数学ematical证明需要非常强的条件，但事实是，这句话与井产生了共鸣
已知的Hebbian原理——神经元一起放电，连接在一起——暗示了潜在的
在实践中，即使在不那么严格的条件下，这个想法也是适用的。
缺点是，现在的计算基础设施在计算数字时效率非常低
非均匀稀疏数据结构的计算。
即使算术运算的数量是
减少了100倍，查找和缓存未命中的开销占了主导地位，所以切换到稀疏
矩阵不会有回报。
随着使用水平的稳步提高，
高度优化的数字库，允许非常快的密集矩阵乘法，利用ing底层CPU或GPU硬件的微小细节[16,9]。
此外,非均匀稀疏
模型需要更复杂的工程和计算基础设施。
最新的视觉
面向对象的机器学习系统利用em利用卷积来利用空间领域的稀疏性。
然而，卷积被实现为密集连接的集合
到较早一层的补丁。
卷积网络传统上使用随机和稀疏连接
表中自[11]以来的特征维数，为了打破对称性和提高学习，
为了更好地优化并行计算，trend恢复了与[9]的全连接。
的
结构的均匀性和大量的过滤器和更大的批大小允许利用
高效密集的计算。

这就提出了一个问题，是否有希望实现下一个中间步骤:一个利用额外稀疏性的体系结构，即使是在滤波器级，就像理论建议的那样，但利用密集矩阵的计算来利用我们现有的硬件。
大量关于稀疏矩阵计算的文献(如[3])表明，将稀疏矩阵聚类为相对密集的子矩阵往往会为稀疏矩阵乘法提供最先进的实际性能。
在不久的将来，类似的方法将被用于非均匀深度学习架构的自动化构建，这种想法似乎并不牵强



先启架构最初是第一作者评估假设的一个案例研究
一个复杂的网络拓扑构造演算法的输出，试图近似一个稀疏的
视觉网络中[2]隐含的结构，并通过密集、阅读ily可用组件覆盖假设结果。
尽管这是一个高度投机性的事业，但只有在两次迭代之后
在对拓扑结构的精确选择上，我们已经可以看到相对于基于[12]的参考体系结构ture的适度收益。
经过进一步的调整学习速度，超参数和改进训练
方法中，我们确定所得到的初始架构在
定位和目标检测上下文作为[6]和[5]的基础网络。
有趣的是,虽然
大多数最初的架构选择都经过了彻底的质疑和测试，结果证明它们至少是局部最优的。
但必须谨慎:尽管提出的架构在计算机领域取得了成功
愿景，它的质量是否可以归因于指导原则仍然是值得怀疑的
引导它的建设。
这需要更彻底的分析和验证:
例如，如果基于下面描述的原则的自动化工具会发现类似的，但是
更好的拓扑结构为视觉网络。
最令人信服的证据是如果一个自动化的
系统将创建网络拓扑结构，从而在其他使用相同拓扑结构的领域中获得类似的收益
算法，但是全局架构看起来非常不同。
至少，最初的成功
初始架构为这个方向上令人兴奋的未来工作提供了坚定的动力。

## 4 Architectural Details

Inception架构的主要思想是基于找出卷积视觉网络中的最优局部稀疏结构是如何被现成的密集组件逼近和覆盖的。
注意，假设平移不变性意味着我们的网络将由卷积构建块构建。
我们所需要的就是找到最优的局部构造，并在空间上重复它。
Arora等人[2]提出了一种逐层构建的方法，即对最后一层的相关统计数据进行分析，并将它们聚成具有高相关性的单元组。
这些簇形成了下一层的单元，并与上一层的单元相连。
我们假设来自前面一层的每个单元对应于输入图像的某个区域，这些单元被分组成滤波器组。
在较低的层(接近输入的层)，相关单元会集中在局部区域。
这意味着，我们最终将会有很多簇集中在一个区域，它们可以被下一层的1×1 convolutions覆盖
建议在[12]。
然而，人们也可以预期会有更少的数量更多
在空间上分散的集群可以被更大的斑块上的卷积覆盖
将是斑块数量的减少，在越来越大的区域。
为了避免patch的对齐问题，当前的Inception体系结构被限制为1×1的过滤器大小，
3×3和5×5，然而，这个决定更多的是基于方便性而不是必要性。
它还
意味着建议的架构是所有这些层及其输出过滤器的组合
银行连接成一个单一的输出向量，形成下一阶段的输入。
此外,
因为合用操作对于当前最先进的卷积技术的成功至关重要
网络，它建议在每个这样的阶段增加一个可选的并行池路径
另外还有一些有益的影响(参见图2(a))。
当这些“初始模块”被堆叠在彼此之上时，它们的输出相关统计信息
一定会发生变化:当更高抽象的特征被更高层次捕捉时，它们的空间性
集中度预计会下降，这表明3×3和5×5的卷积的比例应该会下降
随着我们移动到更高的层次而增加。
以上模块的一个大问题是，至少在这种na¨ıve形式中是这样的，即使是中等数量的
5×5在带有大量数字的卷积层上，卷积运算的代价非常昂贵
的过滤器。
一旦将共用单元添加到混合单元中，这个问题就会变得更加明显:
它们的输出滤波器的数目等于前一阶段滤波器的数目。
的合并
池化层的输出与卷积层的输出将导致不可避免的结果



从一个阶段到另一个阶段增加输出的数量。
即使这个架构可能覆盖
最优稀疏结构，它会做得非常低效，导致内部计算崩溃
几个阶段。
这就引出了所提议的体系结构的第二个想法:在计算需求会增加太多的地方，明智地应用降维和投影。
这是基于嵌入的成功:即使是低维度的嵌入也可能包含很多
关于一个较大的图像补丁的信息。
然而，嵌入表示信息
密集、压缩的表单和压缩的信息更难建模。
我们愿意留下
我们的表示在大多数地方是稀疏的(如[2]条件所要求的)，并压缩
只在需要全部聚集的时候才发送信号。
也就是说，使用1×1卷积
在昂贵的3×3和5×5卷积之前计算缩减。
除了作为还原剂使用外，它们还包括矫正线性激活的使用，这使它们具有双重用途。
的
最终结果如图2(b)所示。
一般来说，先启网络是由上述类型的模块堆叠而成的网络
相互之间，偶尔使用stride 2的最大池层将网格的分辨率减半。
为
由于技术原因(训练期间的内存效率)，开始使用Inception似乎是有益的
只在较高的层上模块，而以传统的卷积方式保持较低的层。
这并不是绝对必要的，只是反映了我们当前一些基础设施的低效
实现。
这个体系结构的一个主要好处是它允许增加
每个阶段的单元明显没有在计算复杂度中失控膨胀。
的
普遍使用的降维允许屏蔽大量的输入滤波器
到下一层的最后一步，在与一个大的卷积之前先缩小它们的维度
补丁的大小。
这种设计的另一个实际有用的方面是，它与直觉一致
视觉信息应在不同的尺度上进行处理，然后进行聚合，以便进行下一阶段
可以同时从不同尺度提取特征。
改进的计算资源使用，可以同时增加每个阶段的宽度
以及不涉及计算困难的阶段数。
另一种方法
利用先启体系结构是为了创建稍差的，但计算上更便宜的版本
它。
我们已经发现，所有包括的旋钮和杠杆允许计算资源的控制平衡，可以导致网络比类似执行的非初始架构的网络快2 - 3倍，但这需要仔细的手工设计。