##  相关工作

图像到图像的转换问题通常被表述为逐像素分类分级或回归(例如，[39,58,28,35,62])。
这些公式将输出空间视为“非结构化的”，也就是说，根据给定的输入图像，在中有条件地考虑每个输出像素。

低频信号 高频信号？局部图像块PATCH https://www.jianshu.com/p/57ff6f96ce4c

该鉴别器有效地将图像建模为马尔可夫随机场，并假设像素之间的独立性大于一个斑块直径。
这种联系早在[38]中就有探讨，也是纹理[17,21]和风格[16,25,22,37]模型中常见的假设。
因此，我们的PatchGAN可以理解为一种纹理/风格的丧失。

为了优化我们的网络，我们遵循[24]的标准方法:在D和G上交替进行一个梯度下降步骤。
正如GAN最初的文章所建议的，我们不是训练G去最小化log(1−D(x, G(x, z))，而是训练log D(x, G(x, z))[24]。
此外，我们在优化D时将目标除以2，这减慢了D相对于g的学习速度。我们使用minibatch SGD和应用Adam求解器[32]，其学习速度为0.0002，动量参数β1 = 0.5， β2 = 0.999。

在推理时，我们以与训练阶段完全相同的方式运行生成器网络。
这与通常的协议不同，因为我们在测试时应用dropout，并且我们使用测试批的统计数据应用批规范化[29]，而不是使用训练批的聚合统计数据。
当批处理大小设置为1时，这种批处理规范化方法被称为“实例正常化”，并且在图像生成任务[54]中被证明是有效的。
在我们的实验中，我们根据实验使用1到10之间的批大小。

为了探索条件gan的通用性，我们在各种任务和数据集上测试了该方法，包括图形任务(如照片生成)和视觉任务(如语义分割):

## 小数据集也很好

我们注意到，即使在很小的数据集上，也经常可以获得不错的re结果。
我们的facade训练集仅包含400张图片(见图14)，而日夜训练集仅包含91个独特的网络摄像头(见图15)。
在这种规模的数据集上，训练可以非常快:例如，图14所示的结果在单个Pascal Titan X GPU上花了不到两个小时的时间进行训练。
在测试时，所有的模型在这个GPU上运行不到一秒。

评价合成图像的质量是[52]的一个开放性和难点问题。
传统的度量标准，如每像素均方误差不评估结果的联合统计，因此不测量结构损失目标捕捉的结构。
为了更全面地评价我们的结果的视觉质量，我们采用了两种策略。
首先，我们在Amazon Mechanical Turk (AMT)上进行“真实vs虚假”感知研究。
对于像着色和照片生成这样的图形问题，对人类观察者来说是否可信通常是最终目标。
因此，我们使用这种方法来测试我们的地图生成、航空照片生成和图像着色。



## 在我们的AMT实验中，我们遵循了[62]的协议:向Turkers展示了一系列的试验，将由我们的算法生成的“真实”图像与“虚假”图像进行对比。
在每次试验中，每张图片出现一秒钟，之后图片就消失了，土耳其人被给予无限的时间来回答哪些是假的。
每一阶段的前10张图片都是练习，Turkers会得到反馈。
主实验的40次试验均未得到反馈。
每个会话一次只测试一个算法，并且Turkers不能完成超过一个session。
envy 50 Turkers评估了每种算法。
与[62]不同的是，我们没有纳入警戒试验。
在我们的彩色实验中，真实的和假的图像是由相同的灰度输入产生的。
对于地图航拍照片，真假图像不是由相同的输入或der生成的，这样可以增加任务的难度，避免地面再现结果。
对于map↔航拍照片，我们在256×256分辨率的图像上进行训练，使用全卷积平移(如上所述)对512 × 512幅图像进行测试，然后对其进行下采样，以256×256分辨率呈现给Turkers。
对于着色，我们在256 × 256分辨率的图像上进行训练和测试，并在相同分辨率下将结果呈现给Turkers。



这种变体导致性能不佳;
检查结果显示，不管输入的照片是什么，生成振荡器都可以产生几乎完全相同的输出。
显然，在这种情况下，损失度量输入和输出之间匹配的质量是很重要的，而且cGAN的性能确实比GAN好得多。
然而，请注意，添加L1项还会鼓励输出尊重输入，因为L1的损失会抵消ground truth之间的距离，即puts正确匹配输入，并合成puts，而put可能不匹配。
相应地，L1+GAN在创建尊重put label maps的真实渲染方面也很有效。
结合所有的术语，L1+cGAN，同样表现良好

分割任务不好



跳跃层简单channel链接



## cyclegan

图像到图像的平移是一类视觉和图形问题，其目标是通过一系列对齐的图像对学习输入图像和输出图像之间的映射关系。
然而，对于许多任务来说，配对训练数据是不可用的。
我们提出了一种方法学习转换图像从源域X到目标域Y在缺乏配对的例子。
我们的目标是学习映射G: X→Y，使来自G(X)的图像分布不清晰从使用敌对损失的分布Y辨别。
由于这个映射是高度欠约束的，我们将它与逆映射F: Y→X结合，并引入一个循环一致性损失来强制F(G(X))≈X(反之亦然)。
在不存在配对训练数据的情况下，给出了定性结果，包括收集样式转换、对象变形、季节转换、照片增强等。
与之前几种方法的定量比较表明了我们方法的优越性。

image-to - image Translation image Translation的思想至少可以追溯到Hertzmann等人的image Analogies[19]，他们在单个输入-输出训练图像对上使用了一个非参数tex真实模型[10]。
最近的方法是使用输入输出测试ples数据集来学习使用cnn的参数化翻译函数(例如:
[33])。
我们的方法建立在“pix2pix”框架的Isola等人的工作[22]，它使用条件生成对抗网络[16]来学习从输入到输出图像的映射。
类似的想法已经应用于各种任务，如从草图[44]或属性和语义布局[25]生成照片。
然而，与前面的工作不同的是，我们学习映射时没有成对的训练示例。



理论上，对抗性训练可以学习映射G和F，产生与目标相同分布的输出
域Y和域X(严格地说，这个规则要求G和F是随机函数)[15]。
然而，只要有足够的容量，网络就可以将同一组输入图像映射到目标域中的任意随机图像排列，其中任何学习到的映射都可以诱导出与目标分布匹配的输出分布分布。
因此，仅仅是对抗性损失并不能保证学习到的函数可以将单个输入xi映射到期望的输出yi。
为了进一步缩小可能映射函数的空间，我们讨论了学习到的映射

因此，我们寻求一种算法，可以学习在没有成对输入输出示例的域之间转换(图2，右)。
我们假设域之间存在某种潜在的关系关系——例如，它们是同一潜在场景的两种不同呈现——并试图了解这种关系。
虽然我们缺乏配对示例形式的监督sion，但我们可以在集合级别上利用super视觉:我们在域X中得到一组图像，在域Y中得到另一组图像。
我们可以训练一个映射G: X→Y，使输出Yˆ= G(X)，
x∈x，与图像y∈y是无法区分的，通过训练对yˆ和y进行分类的ad标记。理论上，这个ob目标可以在yˆ上归纳出一个匹配的输出分布
pdata(y)的经验分布(一般来说，这需要
G是随机的)[16]。
因此，最优的G就转化了
定义域X到定义域Yˆ等于Y的分布。
但是，这样的翻译并不能保证在中单独的输入x和输出y在有意义的情况下成对出现
方法-在中有无穷多个映射G可以在yˆ上得到相同的分布。
此外,在实践中,
我们发现孤立地优化敌对对象是困难的:标准程序经常导致众所周知的模式崩溃问题，所有输入的图像
映射到相同的输出图像，优化失败
[15]取得进展。
这些问题需要为我们的ob目标添加更多的结构。
因此，我们利用翻译的性质
应该是“循环一致”，从这个意义上说，如果我们把late，例如，把一个句子从英语翻译成法语，然后把late it back从法语翻译成英语，我们应该回来吗
在原来的句子[3]。
数学上，如果我们有
翻译器G: X→Y和另一个翻译器F: Y→X，
那么G和F应该互为倒数，两者都是
映射应该是双射。
我们同时训练映射G和F ，并加入一个循环一致性损失[64]，使年龄F(G(x))≈x和G(F(y))≈y
在X和Y领域的对抗性损失就等于我们的全部
目的实现未配对图像到图像的转换。
我们将我们的方法应用于广泛的领域，
包括集合样式转换，对象变形，
季节转换和照片增强。
我们也比较
与以前依赖于手工定义的方法不同
样式和内容的分解，或共享嵌入ding函数，并表明我们的方法优于这些
基线。
我们提供了PyTorch和Torch的实现。
在我们的网站上查看更多结果。
网络架构我们采用我们的架构
生成网络来自Johnson等人的[23]
显示了令人印象深刻的结果，神经风格转移和超级分辨率。
这个网络包含三个卷积，sev一般剩余块[18]，两个小数步长conversation 与步长1
2
和一个将fea映射到RGB的卷积。
我们使用6块128 × 128的图像和9块
用于256×256和更高分辨率的训练图像块。
类似于Johnson等人的[23]，我们使用实例规范化tion[53]。
对于鉴别器网络，我们使用70 × 70
PatchGANs[22, 30, 29]，目的是对是否
70 × 70的重叠图像补丁是真还是假。
这样一个
补丁级鉴别器体系结构参数较少
比一个全图像鉴别器，可以工作任意大小的图像在一个完全卷积的方式[22]。
摘要
我们考虑图像变换问题，其中输入
图像被转换成输出图像。
最近的方法
问题通常是训练前馈卷积神经网络来处理输出图像和地面真实图像之间的逐像素损失。
平行
工作表明，高质量的图像可以通过定义产生
基于高级特征(ex)优化感知损失函数。
我们结合了两种ap方法的优点，并提出使用感知损失函数进行训练
用于图像转换任务的前馈网络。
我们展示了结果
图像样式的传递，其中一个前馈网络训练解决
Gatys等人提出的实时优化问题。
Com与基于优化的方法相比，我们的网络给出了类似的质量的结果，但要快三个数量级。
我们也试验
使用单幅图像的超分辨率，用
知觉的丧失会带来视觉上令人愉悦的结果。

重建的高分辨率图像与真实的高分辨率图像无论是低层次的像素值上，还是高层次的抽象特征上，和整体概念和风格上，都应当接近。所以，根据图像风格转移时的内容损失和风格损失就可以参考使用，在那篇论文中对纹理的重建使用了高层全局信息+底层细节信息。也就是我们所说的感知特征。





